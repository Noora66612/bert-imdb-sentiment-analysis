{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noora66612/bert-imdb-sentiment-analysis/blob/main/bert_imdb_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: BERT IMDB Sentiment Analysis**\n",
        "\n",
        "### **1. Project Overview**\n",
        "This project implements a binary sentiment classification system designed to analyze unstructured user feedback (movie reviews) and determine the underlying emotional tone (**Positive** vs. **Negative**). By leveraging **Transfer Learning**, the system fine-tunes a pre-trained **BERT (Bidirectional Encoder Representations from Transformers)** model to achieve state-of-the-art accuracy on the IMDB dataset.\n",
        "\n",
        "### **2. Business & Technical Context**\n",
        "* **Goal:** To automate the extraction of customer sentiment from large-scale text data, enabling scalable product analytics and user experience research.\n",
        "* **Data Source:** The **IMDB Large Movie Review Dataset**, consisting of 50,000 highly polar reviews (Maas et al., 2011).\n",
        "* **Methodology:**\n",
        "    * **Data Preprocessing:** Implemented a robust pipeline utilizing dynamic tokenization, padding/truncation strategies (max length 512), and custom PyTorch `Dataset` loaders to handle variable-length sequences.\n",
        "    * **Model Architecture:** Utilized `bert-base-uncased` as the backbone encoder. Engineered a downstream classifier by appending a **Dropout layer (p=0.1)** for regularization and a **Linear layer** to map 768-dimensional contextual embeddings to binary logits.\n",
        "    * **Training Strategy:** Employed the **Adam optimizer** with a precise learning rate (`2e-5`) to fine-tune model parameters, preventing catastrophic forgetting while optimizing Cross-Entropy Loss.\n",
        "\n",
        "### **3. Key Results**\n",
        "* Achieved **~95% Accuracy** on the test set after fine-tuning for 3-4 epochs.\n",
        "* Demonstrated robust generalization by monitoring **F1-score**, **Precision**, and **Recall** to ensure balanced performance across classes.\n",
        "* Deployed an inference function capable of predicting sentiment probabilities for unseen, real-world text inputs.\n",
        "\n",
        "### **4. Tech Stack**\n",
        "* **Deep Learning Framework:** PyTorch\n",
        "* **NLP Library:** Hugging Face Transformers (`BertTokenizer`, `BertModel`)\n",
        "* **Data Manipulation:** Pandas, Scikit-learn (Train/Test splits, Metrics)\n",
        "* **Visualization:** Matplotlib (Learning curves)\n",
        "\n",
        "---\n",
        "\n",
        "### **5. References & Acknowledgements**\n",
        "This project builds upon the following academic resources and open-source frameworks:\n",
        "* **Dataset:** Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). [Learning Word Vectors for Sentiment Analysis](https://aclanthology.org/P11-1015.pdf). In *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics*.\n",
        "* **Framework:** Implementation leverages the pre-trained models provided by [Hugging Face](https://huggingface.co/)."
      ],
      "metadata": {
        "id": "QDj-mTQZLIn6"
      },
      "id": "QDj-mTQZLIn6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Environment Setup & Dependencies**\n",
        "Installing the Hugging Face `datasets` and `transformers` libraries, and importing necessary modules for PyTorch and Scikit-learn."
      ],
      "metadata": {
        "id": "o4FUJUuZRBAG"
      },
      "id": "o4FUJUuZRBAG"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "sGLfFjfXYIQ8",
        "collapsed": true
      },
      "id": "sGLfFjfXYIQ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej79ZzPJXyQ5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn.functional as Fun\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "ej79ZzPJXyQ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Utility Functions: Metrics & Checkpointing**\n",
        "Defining helper functions for:\n",
        "* **Evaluation:** Calculating Accuracy, F1-score, Precision, and Recall.\n",
        "* **Model Management:** Saving and loading model weights (Checkpoints) to ensure reproducibility."
      ],
      "metadata": {
        "id": "24z2z4vKNpvM"
      },
      "id": "24z2z4vKNpvM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOSkuPT7XyQ8"
      },
      "outputs": [],
      "source": [
        "# 1. Prediction & Evaluation Helpers\n",
        "def get_pred(logits):\n",
        "    \"\"\"\n",
        "    Decodes model output logits into predicted class labels.\n",
        "    Performs an argmax operation along the last dimension to select the class with the highest score.\n",
        "    \"\"\"\n",
        "    return torch.argmax(logits, dim=1)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "# calculate confusion metrics\n",
        "def cal_metrics(pred, ans):\n",
        "    \"\"\"\n",
        "    Computes classification metrics including Accuracy, F1-score, Recall, and Precision.\n",
        "\n",
        "    Args:\n",
        "        pred (torch.Tensor): Predicted labels from the model.\n",
        "        ans (torch.Tensor): Ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (accuracy, f1, recall, precision)\n",
        "    \"\"\"\n",
        "    # Detach tensors, move to CPU, and convert to NumPy arrays for Scikit-learn compatibility\n",
        "    pred = pred.cpu().numpy()\n",
        "    ans = ans.cpu().numpy()\n",
        "\n",
        "    acc = accuracy_score(ans, pred)\n",
        "    f1 = f1_score(ans, pred)\n",
        "    recall = recall_score(ans, pred)\n",
        "    precision = precision_score(ans, pred)\n",
        "\n",
        "    return acc, f1, recall, precision"
      ],
      "id": "nOSkuPT7XyQ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuLxKpySXyQ-"
      },
      "outputs": [],
      "source": [
        "# 2. Model Persistence Helpers\n",
        "def save_checkpoint(save_path, model):\n",
        "  \"\"\"Saves the model state dictionary to the specified path.\"\"\"\n",
        "  if save_path == None:\n",
        "      return\n",
        "  torch.save(model.state_dict(), save_path)\n",
        "  print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model, device):\n",
        "  \"\"\"Loads model weights from a checkpoint file.\"\"\"\n",
        "  if load_path==None:\n",
        "      return\n",
        "\n",
        "  state_dict = torch.load(load_path, map_location=device)\n",
        "  print(f'Model loaded from <== {load_path}')\n",
        "\n",
        "  model.load_state_dict(state_dict)\n",
        "  return model"
      ],
      "id": "RuLxKpySXyQ-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Data Ingestion & Exploratory Analysis**\n",
        "Downloading the IMDB dataset from Hugging Face Hub and inspecting the data structure to ensure integrity."
      ],
      "metadata": {
        "id": "yEmNAPYRTe5b"
      },
      "id": "yEmNAPYRTe5b"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load the raw dataset\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "r_mBNUdCA410"
      },
      "id": "r_mBNUdCA410",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Inspect data structure\n",
        "dataset"
      ],
      "metadata": {
        "id": "JxuLKpupMejB"
      },
      "id": "JxuLKpupMejB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "fkrrOqXdTaaE"
      },
      "id": "fkrrOqXdTaaE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Data Consolidation & Stratified Split**\n",
        "To ensure a rigorous evaluation pipeline, we consolidate the original splits and perform a custom **Stratified Shuffle-Split**.\n",
        "\n",
        "**Strategy:**\n",
        "* **Merge:** Combine original Train and Test sets into a single corpus.\n",
        "* **Re-split:** Partition into **Train (80%)**, **Validation (10%)**, and **Test (10%)** sets.\n",
        "* **Stratification:** Ensures the positive/negative sentiment ratio remains balanced across all subsets."
      ],
      "metadata": {
        "id": "K3JV06RvPopH"
      },
      "id": "K3JV06RvPopH"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert Hugging Face datasets to Pandas DataFrames\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])\n",
        "\n",
        "# 2. Merge into a single dataframe\n",
        "all_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "# 3. Stratified Split Strategy (8:1:1)\n",
        "# First split: 80% Train, 20% Temp (Val + Test)\n",
        "train_df, test_df = train_test_split(all_df, test_size=0.2, random_state=42, stratify=all_df['label'])\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "PHWieZNXIos5"
      },
      "id": "PHWieZNXIos5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the balance of the consolidated dataset to ensure no class imbalance issues exist (ideally 50/50).\n",
        "all_df.label.value_counts() / len(all_df)"
      ],
      "metadata": {
        "id": "mSQjyAzaoAJD"
      },
      "id": "mSQjyAzaoAJD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Final Partitioning & Persistence**\n",
        "Executing the final split to create **Train (80%)**, **Validation (10%)**, and **Test (10%)** sets. The processed datasets are then saved as TSV files for reproducibility."
      ],
      "metadata": {
        "id": "1GIMI6-5U7Og"
      },
      "id": "1GIMI6-5U7Og"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_data = train_test_split(all_df, random_state=1111, train_size=0.8)\n",
        "dev_df, test_df = train_test_split(temp_data, random_state=1111, train_size=0.5)\n",
        "print('# of train_df:', len(train_df))\n",
        "print('# of dev_df:', len(dev_df))\n",
        "print('# of test_df data:', len(test_df))\n",
        "\n",
        "# save data\n",
        "train_df.to_csv('./train.tsv', sep='\\t', index=False)\n",
        "dev_df.to_csv('./val.tsv', sep='\\t', index=False)\n",
        "test_df.to_csv('./test.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "RhM_od5-Tpkc"
      },
      "id": "RhM_od5-Tpkc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le1XiOWDXyRE"
      },
      "source": [
        "### **6. Custom Dataset & Tokenization Pipeline**\n",
        "Implementing a custom PyTorch `Dataset` class to manage data ingestion. This class integrates the **BERT tokenizer** to transform raw text into model-compatible tensors (Input IDs, Attention Masks) on-the-fly, ensuring efficient memory usage.\n",
        "\n",
        "**Key Features:**\n",
        "* **Dynamic Tokenization:** Handles padding and truncation to a fixed `max_length`.\n",
        "* **Label Encoding:** Supports both binary and one-hot encoding for scalability.\n",
        "* **Mode Handling:** Differentiates between `train/val` (with labels) and `test` (inference mode) splits."
      ],
      "id": "le1XiOWDXyRE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvkBgVIhXyRF"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class for handling BERT input formatting\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, mode, df, specify, args):\n",
        "    # Ensure valid partition: train, val, or test\n",
        "    assert mode in [\"train\", \"val\", \"test\"]\n",
        "    self.mode = mode\n",
        "    self.df = df\n",
        "    self.specify = specify # Column containing the text data to be analyzed\n",
        "    if self.mode != 'test':\n",
        "      self.label = df['label']\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(args[\"config\"])\n",
        "    self.max_len = args[\"max_len\"]\n",
        "    self.num_class = args[\"num_class\"]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  # Encodes labels into one-hot vectors (required for multi-class tasks)\n",
        "  def one_hot_label(self, label):\n",
        "    return Fun.one_hot(torch.tensor(label), num_classes = self.num_class)\n",
        "\n",
        "  # Tokenizes raw text using the pre-trained BERT tokenizer\n",
        "  def tokenize(self, input_text):\n",
        "    tokens = self.tokenizer(\n",
        "      input_text,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      max_length=self.max_len,\n",
        "      return_tensors='pt'  # Return PyTorch tensors\n",
        "  )\n",
        "    # Remove the batch dimension added by return_tensors='pt'\n",
        "    input_ids = tokens['input_ids'].squeeze(0)\n",
        "    attention_mask = tokens['attention_mask'].squeeze(0)\n",
        "    # Handle token_type_ids for models that require them\n",
        "    token_type_ids = tokens['token_type_ids'].squeeze(0) if 'token_type_ids' in tokens else torch.zeros_like(input_ids)\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids\n",
        "\n",
        "  # Retrieves and processes a single data sample\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    sentence = str(self.df[self.specify][index])\n",
        "    ids, mask, token_type_ids = self.tokenize(sentence)\n",
        "\n",
        "\n",
        "    if self.mode == \"test\":\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
        "            torch.tensor(token_type_ids, dtype=torch.long)\n",
        "    else:\n",
        "        if self.num_class > 2:\n",
        "          return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
        "            torch.tensor(token_type_ids, dtype=torch.long), self.one_hot_label(self.label[index])\n",
        "        else:\n",
        "          return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long), \\\n",
        "            torch.tensor(token_type_ids, dtype=torch.long), torch.tensor(self.label[index], dtype=torch.long)"
      ],
      "id": "yvkBgVIhXyRF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Model Architecture: BERT for Sequence Classification**\n",
        "Constructing a custom classifier class inheriting from `BertPreTrainedModel`. This architecture leverages the pre-trained BERT encoder to generate contextual embeddings, which are then passed through a task-specific classification head.\n",
        "\n",
        "**Architecture Breakdown:**\n",
        "1.  **Backbone:** `BertModel` (pre-trained) extracts the 768-dimensional contextual representation for the `[CLS]` token.\n",
        "2.  **Regularization:** A `Dropout` layer (p=0.1) is applied to the pooled output to prevent overfitting during fine-tuning.\n",
        "3.  **Classifier:** A `Linear` layer maps the hidden states to the target class logits (Dimension: `hidden_size` $\\to$ `num_class`).\n",
        "\n",
        "**Forward Pass Strategy:**\n",
        "The model takes input indices and attention masks, passes them through the BERT transformer, extracts the pooled output (sentence embedding), and computes the final logits for classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "RQsY2rjue4Tm"
      },
      "id": "RQsY2rjue4Tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suyj5WLJXyQ7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Custom BERT Classifier Module\n",
        "class BertClassifier(BertPreTrainedModel):\n",
        "  def __init__(self, config, args):\n",
        "    super(BertClassifier, self).__init__(config)\n",
        "    self.bert = BertModel(config)\n",
        "\n",
        "    # Dropout layer for regularization\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    # Classifier Head: Linear layer mapping BERT's hidden size (768) -> num_class (2)\n",
        "    self.classifier = nn.Linear(config.hidden_size, args[\"num_class\"])\n",
        "\n",
        "    # Initialize weights (inherited from BertPreTrainedModel)\n",
        "    self.init_weights()\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
        "              head_mask=None, inputs_embeds=None, labels=None, output_attentions=None,\n",
        "              output_hidden_states=None, return_dict=None):\n",
        "\n",
        "      # 1. BERT Encoder pass\n",
        "      # outputs[0]: last_hidden_state (sequence_output)\n",
        "      # outputs[1]: pooled_output (CLS token embedding, processed by a linear layer + tanh)\n",
        "      outputs = self.bert(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids,\n",
        "          position_ids=position_ids,\n",
        "          head_mask=head_mask,\n",
        "          inputs_embeds=inputs_embeds,\n",
        "          output_attentions=output_attentions,\n",
        "          output_hidden_states=output_hidden_states,\n",
        "          return_dict=return_dict\n",
        "      )\n",
        "\n",
        "      # 2. Feature Extraction (CLS token representation)\n",
        "      pooled_output = outputs[1]  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "      # 3. Classifier Head (Dropout + Linear)\n",
        "      pooled_output = self.dropout(pooled_output)\n",
        "      logits = self.classifier(pooled_output)  # Shape: (batch_size, num_class)\n",
        "\n",
        "      return logits"
      ],
      "id": "suyj5WLJXyQ7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Evaluation Protocol**\n",
        "Defining the evaluation loop to measure model performance on the validation set.\n",
        "\n",
        "**Mechanism:**\n",
        "* **Inference Mode:** Utilizes `model.eval()` to disable training-specific layers like Dropout.\n",
        "* **Resource Optimization:** Wraps operations in `torch.no_grad()` to suppress gradient calculation, reducing memory usage and speeding up inference."
      ],
      "metadata": {
        "id": "fmr97mUu2Zmt"
      },
      "id": "fmr97mUu2Zmt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QVJaIv5XyQ9"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "  \"\"\"\n",
        "    Evaluates the model on a given dataset (validation or test).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The BERT classifier.\n",
        "        data_loader (DataLoader): The dataset to evaluate.\n",
        "        device (torch.device): CPU or GPU.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Averages of (Loss, Accuracy, F1, Recall, Precision) over the dataset.\n",
        "  \"\"\"\n",
        "  val_loss, val_acc, val_f1, val_rec, val_prec = 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "  step_count = 0\n",
        "  loss_fct = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Switch to evaluation mode (disables dropout)\n",
        "  model.eval()\n",
        "\n",
        "  # Disable gradient calculation for inference\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      ids, masks, token_type_ids, labels = [t.to(device) for t in data]\n",
        "\n",
        "      # Forward pass\n",
        "      logits = model(input_ids = ids,\n",
        "              token_type_ids = token_type_ids,\n",
        "              attention_mask = masks)\n",
        "      # Compute metrics\n",
        "      acc, f1, rec, prec = cal_metrics(get_pred(logits), labels)\n",
        "      loss = loss_fct(logits, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      val_acc += acc\n",
        "      val_f1 += f1\n",
        "      val_rec += rec\n",
        "      val_prec += prec\n",
        "      step_count += 1\n",
        "\n",
        "    # Calculate averages\n",
        "    val_loss = val_loss / step_count\n",
        "    val_acc = val_acc / step_count\n",
        "    val_f1 = val_f1 / step_count\n",
        "    val_rec = val_rec / step_count\n",
        "    val_prec = val_prec / step_count\n",
        "\n",
        "  return val_loss, val_acc, val_f1, val_rec, val_prec"
      ],
      "id": "_QVJaIv5XyQ9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAaBnWzTXyQ_"
      },
      "source": [
        "### **9. Hyperparameters & Training Configuration**\n",
        "Establishing the training constraints and initializing the model environment.\n",
        "\n",
        "**Configuration:**\n",
        "* **Batch Size:** 16 (Optimized for standard GPU memory).\n",
        "* **Learning Rate:** 2e-5 (Standard for BERT fine-tuning).\n",
        "* **Epochs:** 4 (Transfer learning typically requires fewer epochs to converge).\n",
        "* **Max Length:** 512 (BERT's maximum sequence length)."
      ],
      "id": "sAaBnWzTXyQ_"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Hyperparameter Configuration\n",
        "parameters = {\n",
        "    \"num_class\": 2,\n",
        "    \"time\": str(datetime.now()).replace(\" \", \"_\"),\n",
        "    # Model config\n",
        "    \"model_name\": 'BERT',\n",
        "    \"config\": 'bert-base-uncased',\n",
        "    \"learning_rate\": 2e-5 ,\n",
        "    \"epochs\": 4,\n",
        "    \"max_len\": 512,\n",
        "    \"batch_size\": 16,\n",
        "    \"dropout\": 0.3,\n",
        "}"
      ],
      "metadata": {
        "id": "Jaui0JY50LR_"
      },
      "id": "Jaui0JY50LR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Data Loaders Instantiation**\n",
        "Reading the processed TSV files and instantiating the PyTorch `DataLoader` objects for batch processing.\n",
        "* **Sampling:** A subset of data (4000 train, 500 val) is sampled here for rapid prototyping.\n",
        "* **Batching:** Data is loaded in batches of 16 (shuffled for training) to optimize stochastic gradient descent."
      ],
      "metadata": {
        "id": "IJpQ20GSZOoV"
      },
      "id": "IJpQ20GSZOoV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0t0NVOAXyRG"
      },
      "outputs": [],
      "source": [
        "# 1. Load training data\n",
        "# Subsampling 4000 samples for rapid prototyping; remove .sample() for full training\n",
        "train_df = pd.read_csv('./train.tsv', sep = '\\t').sample(4000).reset_index(drop=True)\n",
        "train_dataset = CustomDataset('train', train_df, 'text', parameters)\n",
        "train_loader = DataLoader(train_dataset, batch_size=parameters['batch_size'], shuffle=True)\n",
        "\n",
        "# 2. Load validation data\n",
        "val_df = pd.read_csv('./val.tsv', sep = '\\t').sample(500).reset_index(drop=True)\n",
        "val_dataset = CustomDataset('val', val_df, 'text', parameters)\n",
        "val_loader = DataLoader(val_dataset, batch_size=parameters['batch_size'], shuffle=True)"
      ],
      "id": "W0t0NVOAXyRG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. Model Initialization & Optimization Configuration**\n",
        "Setting up the computation device, instantiating the `BertClassifier`, and defining the training components.\n",
        "* **Device Strategy:** Automatically selects CUDA (GPU) if available for acceleration.\n",
        "* **Loss Function:** Cross-Entropy Loss is used for binary classification.\n",
        "* **Optimizer:** Employing **Adam** with a learning rate of `2e-5`, a standard choice for fine-tuning BERT.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nMZ8wEyjab68"
      },
      "id": "nMZ8wEyjab68"
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.logging.set_verbosity_error() # Suppress transformers warnings\n",
        "\n",
        "# 1. Device Selection (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Initialize Model and Loss Function\n",
        "model = BertClassifier.from_pretrained(parameters['config'], parameters).to(device)\n",
        "loss_fct = torch.nn.CrossEntropyLoss() # we use cross entrophy loss\n",
        "\n",
        "# 3. Optimizer Setup\n",
        "# Utilizing Adam optimizer for stable convergence\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=parameters['learning_rate'], betas=(0.9, 0.999), eps=1e-9)"
      ],
      "metadata": {
        "id": "5D7NzvuGaZfY"
      },
      "id": "5D7NzvuGaZfY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. Training Loop & Metric Logging**\n",
        "The training process iterates through the dataset for a specified number of epochs.\n",
        "* **Loss Function:** `CrossEntropyLoss` is utilized as the criterion. Note that `CrossEntropyLoss` in PyTorch expects raw logits (not softmax probabilities) as input.\n",
        "* **Validation:** After each training epoch, the model is evaluated on the validation set to monitor generalization performance.\n",
        "* **Logging:** Loss and classification metrics (Accuracy, F1, Recall, Precision) are recorded at each epoch for subsequent visualization."
      ],
      "metadata": {
        "id": "wtThctN23VIS"
      },
      "id": "wtThctN23VIS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUX_aw7hXyRH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Initialize metric containers\n",
        "metrics = ['loss', 'acc', 'f1', 'rec', 'prec']\n",
        "mode = ['train_', 'val_']\n",
        "record = {s+m :[] for s in mode for m in metrics}\n",
        "\n",
        "for epoch in range(parameters[\"epochs\"]):\n",
        "\n",
        "    st_time = time.time()\n",
        "    train_loss, train_acc, train_f1, train_rec, train_prec = 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "    step_count = 0\n",
        "\n",
        "    # Set model to training mode (enables dropout and batch norm)\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:\n",
        "        ids, masks, token_type_ids, labels = [t.to(device) for t in data]\n",
        "\n",
        "        # Clear previously calculated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute logits\n",
        "        logits = model(input_ids=ids, token_type_ids=token_type_ids, attention_mask=masks)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute batch performance metrics\n",
        "        acc, f1, rec, prec = cal_metrics(get_pred(logits), labels)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc\n",
        "        train_f1 += f1\n",
        "        train_rec += rec\n",
        "        train_prec += prec\n",
        "        step_count += 1\n",
        "\n",
        "    # Validation step: Evaluate performance on the dev set after each epoch\n",
        "    val_loss, val_acc, val_f1, val_rec, val_prec = evaluate(model, val_loader, device)\n",
        "\n",
        "    # Compute average metrics for the current epoch\n",
        "    train_loss = train_loss / step_count\n",
        "    train_acc = train_acc / step_count\n",
        "    train_f1 = train_f1 / step_count\n",
        "    train_rec = train_rec / step_count\n",
        "    train_prec = train_prec / step_count\n",
        "\n",
        "    # Print log\n",
        "    print('[epoch %d] cost time: %.4f s'%(epoch + 1, time.time() - st_time))\n",
        "    print('         loss     acc     f1      rec    prec')\n",
        "    print('train | %.4f, %.4f, %.4f, %.4f, %.4f'%(train_loss, train_acc, train_f1, train_rec, train_prec))\n",
        "    print('val  | %.4f, %.4f, %.4f, %.4f, %.4f\\n'%(val_loss, val_acc, val_f1, val_rec, val_prec))\n",
        "\n",
        "    # Store metrics for visualization\n",
        "    record['train_loss'].append(train_loss)\n",
        "    record['train_acc'].append(train_acc)\n",
        "    record['train_f1'].append(train_f1)\n",
        "    record['train_rec'].append(train_rec)\n",
        "    record['train_prec'].append(train_prec)\n",
        "\n",
        "    record['val_loss'].append(val_loss)\n",
        "    record['val_acc'].append(val_acc)\n",
        "    record['val_f1'].append(val_f1)\n",
        "    record['val_rec'].append(val_rec)\n",
        "    record['val_prec'].append(val_prec)"
      ],
      "id": "BUX_aw7hXyRH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHTX3ycLXyRH"
      },
      "outputs": [],
      "source": [
        "# Save the trained model weights\n",
        "save_checkpoint('./bert.pt' , model)"
      ],
      "id": "YHTX3ycLXyRH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. Learning Curves & Performance Visualization**\n",
        "Visualizing the training dynamics to assess model convergence and generalization.\n",
        "* **Loss Curve:** Monitors the descent of training and validation loss to detect overfitting (e.g., if validation loss starts rising).\n",
        "* **Metric Curves:** Tracks Accuracy and F1-score improvements across epochs."
      ],
      "metadata": {
        "id": "RHzv2n2E5Zex"
      },
      "id": "RHzv2n2E5Zex"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtQra69-XyRI"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 4\n",
        "def draw_pics(record, name, img_save=False, show=False):\n",
        "    \"\"\"\n",
        "    Plots learning curves for a specific metric (e.g., Loss, Accuracy) across epochs.\n",
        "\n",
        "    Args:\n",
        "        record (dict): Dictionary containing metric history.\n",
        "        name (str): Metric name to plot (e.g., 'loss', 'acc', 'f1').\n",
        "        img_save (bool): Whether to save the plot as an image.\n",
        "        show (bool): Whether to display the plot inline.\n",
        "    \"\"\"\n",
        "    x_ticks = range(1, EPOCHS+1)\n",
        "\n",
        "    plt.figure(figsize=(6, 3))\n",
        "\n",
        "    plt.plot(x_ticks, record['train_'+name], '-o', color='lightskyblue',\n",
        "             markeredgecolor=\"teal\", markersize=3, markeredgewidth=1, label = 'Train')\n",
        "    plt.plot(x_ticks, record['val_'+name], '-o', color='pink',\n",
        "             markeredgecolor=\"salmon\", markersize=3, markeredgewidth=1, label = 'Val')\n",
        "    plt.grid(color='lightgray', linestyle='--', linewidth=1)\n",
        "\n",
        "    plt.title('Model', fontsize=14)\n",
        "    plt.ylabel(name, fontsize=12)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.xticks(x_ticks, fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.legend(loc='lower right' if not name.lower().endswith('loss') else 'upper right')\n",
        "\n",
        "    if img_save:\n",
        "        plt.savefig(name+'.png', transparent=False, dpi=300)\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    plt.close()"
      ],
      "id": "HtQra69-XyRI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKEdRFDzXyRI"
      },
      "outputs": [],
      "source": [
        "# Plot Cross-Entropy Loss curve\n",
        "draw_pics(record, 'loss', img_save=False, show=True)"
      ],
      "id": "RKEdRFDzXyRI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gjV5dGCXyRI"
      },
      "outputs": [],
      "source": [
        "# Plot Accuracy curve\n",
        "draw_pics(record, 'acc', img_save=False, show=True)"
      ],
      "id": "-gjV5dGCXyRI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot F1-Score curve\n",
        "draw_pics(record, 'f1', img_save=False, show=True)"
      ],
      "metadata": {
        "id": "6Ej8ffuKDRwZ"
      },
      "id": "6Ej8ffuKDRwZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Recall curve (Sensitivity)\n",
        "draw_pics(record, 'rec', img_save=False, show=True)"
      ],
      "metadata": {
        "id": "F2xU_13LDlWa"
      },
      "id": "F2xU_13LDlWa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Precision curve (Positive Predictive Value)\n",
        "draw_pics(record, 'prec', img_save=False, show=True)"
      ],
      "metadata": {
        "id": "lNDEyPWJDgIS"
      },
      "id": "lNDEyPWJDgIS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXV9nz2RXyRJ"
      },
      "source": [
        "## **14. Inference Pipeline**\n",
        "\n",
        "### **Probability Calculation & Label Mapping**\n",
        "Defining utility functions to convert raw model logits into probabilities using **Softmax**, and mapping numerical indices back to readable class labels (Positive/Negative).\n",
        "\n"
      ],
      "id": "NXV9nz2RXyRJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEjRQrsnXyRJ"
      },
      "outputs": [],
      "source": [
        "def Softmax(x):\n",
        "  \"\"\"Computes the softmax of vector x.\"\"\"\n",
        "  return torch.exp(x) / torch.exp(x).sum()\n",
        "\n",
        "def label2class(label):\n",
        "  \"\"\"Maps an integer label to its corresponding class name.\"\"\"\n",
        "  l2c = {0:'negative', 1:'positive'}\n",
        "  return l2c[label]"
      ],
      "id": "BEjRQrsnXyRJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **15. Single Instance Prediction**\n",
        "The `predict_one` function encapsulates the end-to-end inference lifecycle for a single text query:\n",
        "1.  **Tokenization:** Converts raw text into BERT-compatible tensors (Input IDs, Masks).\n",
        "2.  **Forward Pass:** Computes logits using the trained model in evaluation mode.\n",
        "3.  **Post-processing:** Applies Softmax to interpret logits as probabilities and extracts the predicted class."
      ],
      "metadata": {
        "id": "8xYoz3EW6m7x"
      },
      "id": "8xYoz3EW6m7x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4D4Z9-8XyRK"
      },
      "outputs": [],
      "source": [
        "def predict_one(query, model):\n",
        "    \"\"\"\n",
        "    Predicts the sentiment of a single input sentence.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input text to classify.\n",
        "        model (nn.Module): The fine-tuned BERT classifier.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (probabilities, predicted_class_name)\n",
        "    \"\"\"\n",
        "    # Switch model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize tokenizer from the pretrained configuration\n",
        "    tokenizer = BertTokenizer.from_pretrained(parameters['config'])\n",
        "\n",
        "    # 1. Tokenize the input text with padding and truncation\n",
        "    inputs = tokenizer(\n",
        "        query,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=parameters['max_len'],\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # 2. Move input tensors to the computation device (GPU/CPU)\n",
        "    input_ids = inputs['input_ids'].to(model.device)\n",
        "    attention_mask = inputs['attention_mask'].to(model.device)\n",
        "    token_type_ids = inputs['token_type_ids'].to(model.device)\n",
        "\n",
        "    # 3. Perform inference (Forward pass) without gradient calculation\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids)\n",
        "\n",
        "    # 4. Apply Softmax to get class probabilities\n",
        "    # logits[0] selects the first (and only) sample in the batch\n",
        "    probs = Softmax(logits[0]).cpu().numpy()  # logits[0] 是 batch 中第 0 個（因為只有一個輸入）\n",
        "\n",
        "    # 5. Determine the predicted class index (Argmax) and map to label\n",
        "    pred_label = int(torch.argmax(logits, dim=1).cpu().numpy()[0])\n",
        "    pred = label2class(pred_label)\n",
        "\n",
        "    return probs, pred"
      ],
      "id": "a4D4Z9-8XyRK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **16. Model Deployment & Sanity Check**\n",
        "Reloading the best-performing model weights from the saved checkpoint (`bert.pt`) to ensure the inference environment is independent of the training loop."
      ],
      "metadata": {
        "id": "GhNkV6FJhHo8"
      },
      "id": "GhNkV6FJhHo8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egfecB7sXyRK"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# 1. Initialize device and model architecture\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "init_model = BertClassifier.from_pretrained(parameters['config'], parameters) # Initialize architecture\n",
        "\n",
        "# 2. Load fine-tuned weights from the checkpoint file\n",
        "model = load_checkpoint('./bert.pt', init_model, device).to(device)"
      ],
      "id": "egfecB7sXyRK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BowzmKvXyRK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Run a sanity check on a single example to verify inference latency\n",
        "probs, pred = predict_one(\"This movie doesn't attract me\", model)\n",
        "print(pred)"
      ],
      "id": "0BowzmKvXyRK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **17. Batch Inference Strategy**\n",
        "Defining a `predict` function designed to handle large-scale data via `DataLoader`. This allows for efficient batch processing of the entire test set without memory overflow."
      ],
      "metadata": {
        "id": "jW7WgMK-5UXJ"
      },
      "id": "jW7WgMK-5UXJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0s5Da92XyRN"
      },
      "outputs": [],
      "source": [
        "def predict(data_loader, model):\n",
        "  \"\"\"\n",
        "    Performs batch inference on a dataset using a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        data_loader (DataLoader): The iterable data loader containing test samples.\n",
        "        model (nn.Module): The fine-tuned BERT classifier.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of probabilities, list of predicted labels)\n",
        "  \"\"\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(parameters['config'])\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  total_probs, total_pred = [], []\n",
        "\n",
        "  # Switch model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      # Move batch data to the computation device\n",
        "      input_ids, attention_mask, \\\n",
        "      token_type_ids = [t.to(device) for t in data]\n",
        "\n",
        "      # Forward pass: Compute logits\n",
        "      logits = model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "      # Apply Softmax to get class probabilities\n",
        "      probs = Softmax(logits)\n",
        "\n",
        "      # Determine predicted class index (0 or 1)\n",
        "      label_index = torch.argmax(probs[0], dim=0)\n",
        "      pred = label_index.item()\n",
        "\n",
        "      total_probs.append(probs)\n",
        "      total_pred.append(pred)\n",
        "\n",
        "  return total_probs, total_pred"
      ],
      "id": "c0s5Da92XyRN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **18. Execution & Submission Generation**\n",
        "Loading the test dataset, executing batch inference, and aggregating results. The final predictions are saved to `result.tsv` for submission or further analysis."
      ],
      "metadata": {
        "id": "4sjI4V2YiAHD"
      },
      "id": "4sjI4V2YiAHD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPkXV9J5XyRN"
      },
      "outputs": [],
      "source": [
        "# 1. Load and sample testing data\n",
        "# Subsampling 500 instances for demonstration\n",
        "test_df = pd.read_csv('./test.tsv', sep = '\\t').sample(500).reset_index(drop=True)\n",
        "test_dataset = CustomDataset('test', test_df, 'text', parameters)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 2. Execute Prediction\n",
        "total_probs, total_pred = predict(test_loader, model)\n",
        "\n",
        "# 3. Aggregate Results\n",
        "res = test_df.copy()\n",
        "res['pred'] = total_pred # Append predictions to the dataframe\n",
        "\n",
        "# 4. Save results to disk\n",
        "res.to_csv('./result.tsv', sep='\\t', index=False)"
      ],
      "id": "lPkXV9J5XyRN"
    },
    {
      "cell_type": "code",
      "source": [
        "res.head(5)"
      ],
      "metadata": {
        "id": "e8R4lQHVCBw7"
      },
      "id": "e8R4lQHVCBw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy on the test set\n",
        "correct = 0\n",
        "for idx, pred in enumerate(res['pred']):\n",
        "  if pred == res['label'][idx]:\n",
        "    correct += 1\n",
        "print('test accuracy = %.4f'%(correct/len(test_df)))"
      ],
      "metadata": {
        "id": "FDwvwbIokiq-"
      },
      "id": "FDwvwbIokiq-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4QTsH978XyRG",
        "sAaBnWzTXyQ_",
        "IJpQ20GSZOoV",
        "nMZ8wEyjab68",
        "RHzv2n2E5Zex",
        "A6GLT9fOXyRJ"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}